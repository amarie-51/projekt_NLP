{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate_speech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN91u7T5PwlyiD2IrNRlEv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarie-51/projekt_NLP/blob/main/hate_speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGfYyjJielh1"
      },
      "source": [
        "### ** TASK 1**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiPgLxUh2J-4"
      },
      "source": [
        "Przetwarzanie wstępne "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGCATKcwt6lZ"
      },
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxSlBF9wI1H"
      },
      "source": [
        "Tworzymy dataframe z treścią tweeta oraz oceną czy jest hejtem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGCJVpBYtUmS"
      },
      "source": [
        "tags_poprawne=pd.read_csv('/content/test_set_clean_only_tags.txt', header = None)\n",
        "tweets=open('/content/test_set_clean_only_text.txt')\n",
        "table_of_tweets=[]\n",
        "for line in tweets.readlines():\n",
        "  table_of_tweets.append(line)\n",
        "\n",
        "tags_poprawne.columns=[\"Czy hate\"]\n",
        "tweets=pd.DataFrame(table_of_tweets)\n",
        "tweets.columns=[\"Tweet\"]\n",
        "df_tweets=pd.concat([tweets,tags_poprawne],axis=1)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "derscyyHt6BO"
      },
      "source": [
        "df_tweets.loc[:,\"Tweet\"] = df_tweets.Tweet.apply(lambda x : str.lower(x))\n",
        "df_tweets.loc[:,\"Tweet\"] = df_tweets.Tweet.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc5yQ3bC-NEV"
      },
      "source": [
        "Dodać txt ze stop_words, które trzeba usunąć"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh-ga2MI2N5V"
      },
      "source": [
        "Usuwam stop_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpu2NFQi7ptZ"
      },
      "source": [
        "with open(\"/content/stop_words.txt\", \"r\") as stop_words:\n",
        "\tlines = stop_words.read().splitlines()\n",
        "\n",
        "\n",
        "stop_words=lines\n",
        "def remove_stopWords(s):\n",
        "    '''For removing stop words\n",
        "    '''\n",
        "    s = ' '.join(word for word in s.split() if word not in stop_words)\n",
        "    return s\n",
        "\n",
        "df_tweets.loc[:,\"Tweet\"] = df_tweets.Tweet.apply(lambda x: remove_stopWords(x))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh--viuL6DRR"
      },
      "source": [
        "df_tweets[\"Tokeny\"]=df_tweets[\"Tweet\"]\n",
        "df_tweets['Tokeny'] = df_tweets.apply(lambda row: nltk.word_tokenize(row['Tweet']), axis=1)\n",
        "print(df_tweets.columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D690G2grMGZ3"
      },
      "source": [
        "Wizualizacja słów,\n",
        " w razie gdyby pojawiły się jeszcze słowa, które warto usunąć np. xd, dodaje do stop_words\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbW0GWGfC1Nh"
      },
      "source": [
        "def rozklad_slow(data):\n",
        "  all_words = ' '.join([text for text in data ])\n",
        "  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "rozklad_slow(df_tweets[\"Tweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgF42heM1OPm"
      },
      "source": [
        "sprawdzamy najczestsze słowa w tweetach pozytywnych oraz później w negatywnych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH2AudQS0qOP"
      },
      "source": [
        "df_negatywne=df_tweets.loc[df_tweets[\"Czy hate\"]==1].reset_index()\n",
        "\n",
        "rozklad_slow(df_negatywne['Tweet'])\n",
        "\n",
        "df_negatywne_random=df_negatywne.sample(n=100)\n",
        "df_negatywne=pd.concat([df_negatywne,df_negatywne,df_negatywne_random]) #robię trochę manualny oversampling, tworze nowy dataframe z podwojana liczba tweetów negatywnych\n",
        "print(df_negatywne[\"Tweet\"].head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2N_ea_c1rri"
      },
      "source": [
        "df_pozytywne=df_tweets.loc[df_tweets[\"Czy hate\"]==0].reset_index()\n",
        "\n",
        "rozklad_slow(df_pozytywne[\"Tweet\"])\n",
        "print(df_pozytywne[\"Tweet\"].head)\n",
        "df_random_pozytywne=df_pozytywne.sample(n=600)#do nowego dataframe, downsampling tweetów negatywnych"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ROCNuLSF6F"
      },
      "source": [
        "df_tweets=pd.concat([df_random_pozytywne, df_negatywne])\n",
        "\n",
        "print(df_tweets[\"Czy hate\"].head)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk85z3w36x0y"
      },
      "source": [
        "najpopularniejsze słowa wg rodzaju tweeta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnVwqZCs6xek"
      },
      "source": [
        "from collections import Counter\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_most_common(df,top=20):\n",
        "  words=Counter(\" \".join(df).split()).most_common(top)\n",
        "  print(f'Found {len(words)} words.')\n",
        "  most_common = Counter(words).most_common(top)\n",
        "  labels = []\n",
        "  values = []\n",
        "  for label, val in most_common:\n",
        "    labels.append(label)\n",
        "    values.append(val)\n",
        "  return values, labels\n",
        "\n",
        "def plot_most_common(df, top=20, title=''):\n",
        "  values, labels = get_most_common(df, top)\n",
        "  x = range(len(values))\n",
        "\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  plt.bar(x, values)\n",
        "  plt.xticks(x, labels, rotation=90)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "plot_most_common(tweets_train, top=30, title='Top 30 most common words in train set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL67yt0c63kb"
      },
      "source": [
        "# lemmatyzacja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnlZd9JK62bI"
      },
      "source": [
        "!pip install --upgrade spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZfHDlscD78-"
      },
      "source": [
        "!sudo python -m spacy download pl_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjFQcoSk7SOx"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.pl.stop_words import STOP_WORDS as stop_words_spacy\n",
        "\n",
        "nlp=spacy.load('pl_core_news_sm')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBMJQlNJ7nAU"
      },
      "source": [
        "def lemmatize(comments: list, nlp: spacy) -> list:\n",
        "  \n",
        "  def is_redundant(token):\n",
        "    return any([token.is_punct, token.is_stop, token.is_space])\n",
        "\n",
        "  return [' '.join([token.lemma_ for token in nlp(comment) if not is_redundant(token)]) for comment in comments]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqPd4YL1z_7_"
      },
      "source": [
        "podział zbioru tweetów na dwa datasety"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s4U4ru9OOl7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tweets = df_tweets['Tweet'].values\n",
        "label=df_tweets['Czy hate'].values\n",
        "\n",
        "tweets_train, tweets_test, label_train, label_test = train_test_split(tweets, label, test_size=0.2, shuffle=True)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPuLcsftPQCv"
      },
      "source": [
        "oversampling, downsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXY6oikYPPqz"
      },
      "source": [
        "!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vRYM2OfPYrQ"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kq8hVOH718e"
      },
      "source": [
        "# lematyzacja wytrenowanego modelu \n",
        "lematyzacja poprawiła recall i f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fmAxOMG71fT"
      },
      "source": [
        "tweets_test=lemmatize(tweets_test, nlp)\n",
        "tweets_train = lemmatize(tweets_train, nlp)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug0Z30wKGd8U"
      },
      "source": [
        "rozklad_slow(tweets_train) #rozkład słów poddanych lematyzacji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGw8WO1_IJwW"
      },
      "source": [
        "Wektoryzacja tekstu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vb4jWpvSqe1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(tweets_train)\n",
        "\n",
        "X_train = vectorizer.transform(tweets_train)\n",
        "X_test  = vectorizer.transform(tweets_test)\n",
        "print(X_train.shape,X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgJ_nZcGIiOZ"
      },
      "source": [
        "regresja liniowa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaSvIri2S6ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c957fd6-a2b3-48fd-82c1-ae53c8026fca"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, label_train)\n",
        "score = classifier.score(X_test, label_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9536082474226805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFntY5TpKOVW"
      },
      "source": [
        "# Sieci neuronowe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ApuxRcUK6j"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Embedding,LSTM\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLrO27FZVHEc"
      },
      "source": [
        "\n",
        "model= Sequential([\n",
        "                   Dense(units=16,activation=\"relu\"),\n",
        "                   Dropout(0.5),\n",
        "                   Dense(units=8,activation=\"relu\"),\n",
        "                   Dense(units=1, activation=\"sigmoid\")\n",
        "                  \n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fn8JeR1JZcK"
      },
      "source": [
        "model.build(X_train.shape)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLLn3L5eUh6-"
      },
      "source": [
        "history= model.fit(X_train,label_train, validation_data=(X_test,label_test), batch_size=8, epochs=10)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model train vs validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pQXuk5SVDY-"
      },
      "source": [
        "tf.keras.backend.clear_session() #wyczysczenie wytrenowanego modelu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lCgixxw_VD6"
      },
      "source": [
        "model.evaluate(X_test,label_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l63o-CPU6lS"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, label_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, label_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr5z7iyNo1eB"
      },
      "source": [
        "Wykres przedstawiajacy  loss oraz accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJMD-YbPVF7p"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training accuracy')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK4CI6KtVM3I"
      },
      "source": [
        "plot_history(history) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3eLRjcKVni"
      },
      "source": [
        "Sprawdzamy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wbdATnBI4co"
      },
      "source": [
        "y_pred=model.predict(X_test).reshape(-1)\n",
        "print(y_pred[:20])\n",
        "\n",
        "y_pred=np.round(y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o402teAGJogv"
      },
      "source": [
        "print(y_pred[:10])\n",
        "print(label_test[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVo0JyF5I9e1"
      },
      "source": [
        "Ponieważ zbiór danych nie jest równy, dlatego accuracy nie jest dobrym wyznacznikiem modelu.\n",
        "Wartość recall oraz f1-score są niskie, co świadczy o słabym działaniu modelu.\n",
        "Większość tweetów to sa jednak tweety neutralne.\n",
        "Tylko 30% to tweety negatywne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRblJwrDJ3B3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(classification_report(label_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}