{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate_speech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSfeMVxSD4OJTE1bLCn5Bl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarie-51/projekt_NLP/blob/main/hate_speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TUatJYdspJj"
      },
      "source": [
        "!pip install -q -U tensorflow-text\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkrYAWwpksZj"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGfYyjJielh1"
      },
      "source": [
        "### ** TASK 1**\n",
        " Precision, Recall, Balanced F-score and Accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiPgLxUh2J-4"
      },
      "source": [
        "Przetwarzanie wstępne "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGCATKcwt6lZ"
      },
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxSlBF9wI1H"
      },
      "source": [
        "Tworzymy dataframe z treścią tweeta oraz oceną czy jest hejtem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGCJVpBYtUmS"
      },
      "source": [
        "tags_poprawne=pd.read_csv('/content/test_set_clean_only_tags.txt', header = None)\n",
        "tweets=open('/content/test_set_clean_only_text.txt')\n",
        "table_of_tweets=[]\n",
        "for line in tweets.readlines():\n",
        "  table_of_tweets.append(line)\n",
        "\n",
        "tags_poprawne.columns=[\"Czy hate\"]\n",
        "tweets=pd.DataFrame(table_of_tweets)\n",
        "tweets.columns=[\"Tweet\"]\n",
        "df_tweets=pd.concat([tweets,tags_poprawne],axis=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "derscyyHt6BO"
      },
      "source": [
        "df_tweets.loc[:,\"Tweet\"] = df_tweets.Tweet.apply(lambda x : str.lower(x))\n",
        "df_tweets.loc[:,\"Tweet\"] = df_tweets.Tweet.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc5yQ3bC-NEV"
      },
      "source": [
        "Dodać txt ze stop_words, które trzeba usunąć"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh-ga2MI2N5V"
      },
      "source": [
        "Usuwam stop_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpu2NFQi7ptZ"
      },
      "source": [
        "with open(\"/content/stop_words.txt\", \"r\") as stop_words:\n",
        "\tlines = stop_words.read().splitlines()\n",
        "\n",
        "\n",
        "stop_words=lines\n",
        "def remove_stopWords(s):\n",
        "    '''For removing stop words\n",
        "    '''\n",
        "    s = ' '.join(word for word in s.split() if word not in stop_words)\n",
        "    return s\n",
        "\n",
        "df_tweets.loc[:,\"Tweet\"] = df_tweets.Tweet.apply(lambda x: remove_stopWords(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh--viuL6DRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9ba53c-b272-4220-c179-53a07ed3e257"
      },
      "source": [
        "df_tweets[\"Tokeny\"]=df_tweets[\"Tweet\"]\n",
        "df_tweets['Tokeny'] = df_tweets.apply(lambda row: nltk.word_tokenize(row['Tweet']), axis=1)\n",
        "print(df_tweets.columns)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Tweet', 'Czy hate', 'Tokeny'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D690G2grMGZ3"
      },
      "source": [
        "Wizualizacja słów,\n",
        " w razie gdyby pojawiły się jeszcze słowa, które warto usunąć np. xd, dodaje do stop_words\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbW0GWGfC1Nh"
      },
      "source": [
        "def rozklad_slow(data):\n",
        "  all_words = ' '.join([text for text in data ])\n",
        "  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "rozklad_slow(df_tweets[\"Tweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgF42heM1OPm"
      },
      "source": [
        "sprawdzamy najczestsze słowa w tweetach pozytywnych oraz później w negatywnych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH2AudQS0qOP"
      },
      "source": [
        "df_negatywne=df_tweets.loc[df_tweets[\"Czy hate\"]==1].reset_index()\n",
        "\n",
        "rozklad_slow(df_negatywne['Tweet'])\n",
        "print(df_negatywne[\"Tweet\"].head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2N_ea_c1rri"
      },
      "source": [
        "df_pozytywne=df_tweets.loc[df_tweets[\"Czy hate\"]==0].reset_index()\n",
        "\n",
        "rozklad_slow(df_pozytywne[\"Tweet\"])\n",
        "print(df_pozytywne[\"Tweet\"].head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ES3QgHX3Ffq"
      },
      "source": [
        "Trenowanie sieci neuronowych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqPd4YL1z_7_"
      },
      "source": [
        "regresja liniowa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s4U4ru9OOl7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tweets = df_tweets['Tweet'].values\n",
        "label=df_tweets['Czy hate'].values\n",
        "\n",
        "tweets_train, tweets_test, label_train, label_test = train_test_split(tweets, label, test_size=0.2, shuffle=True)\n"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vb4jWpvSqe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb81f61-c64a-42f6-986d-0495a75cee84"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(tweets_train)\n",
        "\n",
        "X_train = vectorizer.transform(tweets_train)\n",
        "X_test  = vectorizer.transform(tweets_test)\n",
        "print(X_train.shape,X_test.shape)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(800, 3373) (200, 3373)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaSvIri2S6ya"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, label_train)\n",
        "score = classifier.score(X_test, label_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFntY5TpKOVW"
      },
      "source": [
        "# Sieci neuronowe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ApuxRcUK6j"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLrO27FZVHEc"
      },
      "source": [
        "model= Sequential([\n",
        "                   Dense(64,input_shape=(X_train.shape),activation=\"relu\"),\n",
        "                   Dropout(0.5),\n",
        "                   Dense(units=32,activation=\"relu\"),\n",
        "                   Dropout(0.5),\n",
        "                   Dense(units=16,activation=\"relu\"),\n",
        "                   Dense(units=1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLLn3L5eUh6-"
      },
      "source": [
        "history= model.fit(X_train,label_train, validation_data=(X_test,label_test), batch_size=8, epochs=10)\n",
        "\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('model train vs validation loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'validation'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pQXuk5SVDY-"
      },
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lCgixxw_VD6"
      },
      "source": [
        "model.evaluate(X_test,label_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l63o-CPU6lS"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, label_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, label_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJMD-YbPVF7p"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training accuracy')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK4CI6KtVM3I"
      },
      "source": [
        "plot_history(history) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3eLRjcKVni"
      },
      "source": [
        "Sprawdzamy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wbdATnBI4co"
      },
      "source": [
        "y_pred=model.predict(X_test).reshape(-1)\n",
        "print(y_pred[10:20])\n",
        "\n",
        "y_pred=np.round(y_pred)\n",
        "print(y_pred[10:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o402teAGJogv"
      },
      "source": [
        "print(label_test[10:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRblJwrDJ3B3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(classification_report(label_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
